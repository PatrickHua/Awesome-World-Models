# Awesome-World-Models
This repository is a collection of research papers on World Models. It aims to provide a useful resource for those interested in this field.

>World Models are a class of models in the field of artificial intelligence that aim to create a simplified, internal representation of the external world. These models are designed to predict the future state of the environment based on current observations and past experiences, allowing an agent to make informed decisions.

## World Model Papers
1. **Learning to Model the World with Language.** arxiv 2023. [paper](https://arxiv.org/pdf/2308.01399.pdf)

    *Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, Anca Dragan.*

   >language helps agents predict the future

2. **Unifying (Machine) Vision via Counterfactual World Modeling.** arxiv 2023. [paper](https://arxiv.org/pdf/2306.01828.pdf)

    *Bear, Daniel M., Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, and Daniel LK Yamins.*

3. **World Models** NIPS 2018. [paper](https://arxiv.org/pdf/1803.10122.pdf) [demo](https://worldmodels.github.io/)

    *Ha, David, and Jürgen Schmidhuber.*

4. **A Control-Centric Benchmark for Video Prediction.** ICLR 2023. [paper](https://arxiv.org/pdf/2304.13723.pdf)

    *Tian, Stephen, Chelsea Finn, and Jiajun Wu.*

5. **Transformers are sample efficient world models.** ICLR 2023. [paper](https://arxiv.org/pdf/2209.00588.pdf)
    *Micheli, Vincent, Eloi Alonso, and François Fleuret.*

6. **Towards Efficient World Models** ICML 2023 Workshops. [paper](https://openreview.net/pdf?id=o8IDoZggqO)
    *Eloi Alonso, Vincent Micheli, and François Fleuret.*

7. **Learning latent dynamics for planning from pixels.** PMLR 2019. [paper](https://arxiv.org/pdf/1811.04551.pdf)
   *Hafner, Danijar, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.*
   
## Video Model Papers
1. **MAGVIT: Masked Generative Video Transformer.** CVPR 2023. [paper](https://arxiv.org/pdf/2212.05199.pdf) [demo](https://magvit.cs.cmu.edu/) [code](https://github.com/google-research/magvit)

   *Yu, Lijun, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann et al.*

   >3d VQ + MaskGIT = 37fps on v100 sampling
   

2. **Diffusion Models for Video Prediction and Infilling.** TMLR 2022. [paper](https://arxiv.org/pdf/2206.07696.pdf)   [code](https://github.com/Tobi-r9/RaMViD)

   *Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi*

3. **Unsupervised Learning for Physical Interaction through Video Prediction** Neurips 2016. [paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/d9d4f495e875a2e075a1a4a6e1b9770f-Paper.pdf)

   *Finn, Chelsea, Ian Goodfellow, and Sergey Levine.*

4. **Unsupervised Learning of Video Representations using Lstms.** ICML 2015. [paper](https://arxiv.org/pdf/1502.04681v3.pdf)
   *Srivastava, Nitish, Elman Mansimov, and Ruslan Salakhudinov.*


## Action Model Papers
1. **Decision Transformer: Reinforcement Learning via Sequence Modeling** Neurips 2021. [paper](https://arxiv.org/pdf/2106.01345.pdf)

   *Chen, Lili, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.*


2. **Diffusion Policy: Visuomotor Policy Learning via Action Diffusion** RSS 2023. [paper](https://arxiv.org/pdf/2303.04137.pdf) [demo](https://diffusion-policy.cs.columbia.edu/)
   *Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran*





